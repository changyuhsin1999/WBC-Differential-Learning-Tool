{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNQGPkaXjAFMaVkj9qtTxVc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/changyuhsin1999/WBC-Differential-Learning-Tool/blob/main/SVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uc13zt5wduD8",
        "outputId": "7412e730-780d-4621-fead-b7d09ccee0ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove './sample_data': No such file or directory\n",
            "fatal: destination path 'WBC-Differential-Learning-Tool' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Remove Colab default sample_data\n",
        "!rm -r ./sample_data\n",
        "\n",
        "# Clone GitHub files to colab workspace\n",
        "repo_name = \"WBC-Differential-Learning-Tool\" # Enter repo name\n",
        "git_path = 'https://github.com/changyuhsin1999/WBC-Differential-Learning-Tool.git'\n",
        "!git clone \"{git_path}\"\n",
        "data_file = \"/content/WBC-Differential-Learning-Tool/data\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms"
      ],
      "metadata": {
        "id": "wXnUyO8ofAHr"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def define_transforms():\n",
        "    \"\"\"\n",
        "    Define transformations for training, validation, and test data.\n",
        "    For training data we will do resize to 224 * 224, randomized horizontal flipping, rotation, lighting effects, and normalization. \n",
        "    For test and val set we will do only center cropping to get to 224 * 224 and normalization\n",
        "    \"\"\"\n",
        "\n",
        "    data_transforms = transforms.Compose([\n",
        "            transforms.Resize(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    return data_transforms"
      ],
      "metadata": {
        "id": "ZN7WM2mZfg-v"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_percentage = 0.6\n",
        "val_percentage = 0.15\n",
        "test_percentage = 0.25\n",
        "\n",
        "batch_size = 8\n",
        "num_workers = 2\n",
        "\n",
        "input_size = 1209600\n",
        "learning_rate = 0.0001\n",
        "momentum = 0.1\n",
        "num_epochs=50"
      ],
      "metadata": {
        "id": "8VudDFYtgIqU"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_datasets(data_dir, train_percentage, val_percentage):\n",
        "    \"\"\"\n",
        "    Create datasets for training, validation, and test\n",
        "\n",
        "    Args:\n",
        "        data_dir (str): path to data directory\n",
        "        train_percentage (float): percentage of data to use for training\n",
        "        val_percentage (float): percentage of data to use for validation\n",
        "\n",
        "    Returns:\n",
        "        train_dataset (torchvision.datasets.ImageFolder): training dataset\n",
        "        val_dataset (torchvision.datasets.ImageFolder): validation dataset\n",
        "        test_dataset (torchvision.datasets.ImageFolder): test dataset\n",
        "        class_names (list): list of class names\n",
        "        num_classes (int): number of classes\n",
        "    \"\"\"\n",
        "    ## Define transformations for training, validation, and test data\n",
        "    data_transforms = define_transforms()\n",
        "\n",
        "    ## Create Datasets for training, testing and validation sets\n",
        "    image_dataset = torchvision.datasets.ImageFolder(root=data_file, transform=data_transforms)\n",
        "    train_size = int(train_percentage * len(image_dataset))\n",
        "    val_size = int(val_percentage * len(image_dataset))\n",
        "    test_size = len(image_dataset) - train_size - val_size\n",
        "\n",
        "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(image_dataset, [train_size, val_size, test_size])\n",
        "\n",
        "    ## get class names associated with labels\n",
        "    class_names = image_dataset.classes\n",
        "    num_classes = len(class_names)\n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset, class_names, num_classes"
      ],
      "metadata": {
        "id": "24Rf-ho9frGp"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloaders(train_dataset, val_dataset, test_dataset, batch_size, num_workers=2):\n",
        "    \"\"\"\n",
        "    Create dataloaders for training and validation and testing sets\n",
        "\n",
        "    Args:\n",
        "        train_dataset (torchvision.datasets.ImageFolder): training dataset\n",
        "        val_dataset (torchvision.datasets.ImageFolder): validation dataset\n",
        "        test_dataset (torchvision.datasets.ImageFolder): test dataset\n",
        "        batch_size (int): batch size\n",
        "        num_workers (int): number of workers to use for dataloader\n",
        "\n",
        "    Returns:\n",
        "        dataloaders (dict): dictionary of dataloaders for training and validation sets\n",
        "        dataset_sizes (dict): dictionary of sizes of training and validation sets\n",
        "    \"\"\"\n",
        "     \n",
        "    ## Create DataLoaders for training, testing and validation sets\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \n",
        "                                            shuffle=True, num_workers=num_workers)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, \n",
        "                                            shuffle=False, num_workers=num_workers)\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, \n",
        "                                            shuffle=True, num_workers=num_workers)\n",
        "\n",
        "    ## Set up dict for dataloaders\n",
        "    dataloaders = {'train':train_loader, 'val':val_loader, 'test': test_loader}\n",
        "\n",
        "    ## Store size of training and validation sets\n",
        "    dataset_sizes = {'train': len(train_dataset), 'val': len(val_dataset), 'test': len(test_dataset)}\n",
        "\n",
        "    return dataloaders, dataset_sizes"
      ],
      "metadata": {
        "id": "KoGtizO3gGdv"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, val_dataset, test_dataset, class_names, num_classes = create_datasets(data_file, train_percentage, val_percentage)\n",
        "dataloaders, dataset_sizes = create_dataloaders(train_dataset, val_dataset, test_dataset, batch_size, num_workers)"
      ],
      "metadata": {
        "id": "6nXNq_wEg7jK"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, input_size, criterion, optimizer, dataloaders, batch_size, device=\"cpu\", num_epochs=1):\n",
        "    \"\"\"\n",
        "    Train the model using transfer learning\n",
        "    Args:\n",
        "        model (torchvision.models): model to train\n",
        "        input_size (int): input size of the model\n",
        "        criterion (torch.nn.modules.loss): loss function\n",
        "        optimizer (torch.optim): optimizer\n",
        "        dataloaders (dict): dictionary of dataloaders for training and validation sets\n",
        "        device (torch.device): device to train on\n",
        "        num_epochs (int): number of epochs to train for\n",
        "    Returns:\n",
        "        model (torchvision.models): trained model\n",
        "    \"\"\"\n",
        "    ## Load the model to GPU if available\n",
        "    model = model.to(device)\n",
        "\n",
        "    ## Train the model\n",
        "    for epoch in range(num_epochs):\n",
        "        avg_loss_epoch = 0\n",
        "        batch_loss = 0\n",
        "        total_batches = 0\n",
        "\n",
        "        for images, labels in dataloaders[\"train\"]:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            images = images.reshape(-1, input_size)\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            ## Forward pass        \n",
        "            outputs = model(images)           \n",
        "            loss_svm = criterion(outputs, labels, batch_size)    \n",
        "            \n",
        "            ## Backward and optimize\n",
        "            loss_svm.backward()\n",
        "            optimizer.step()    \n",
        "            total_batches += 1     \n",
        "            batch_loss += loss_svm.item()\n",
        "\n",
        "        ## Print loss every few iterations\n",
        "        avg_loss_epoch = batch_loss/total_batches\n",
        "        print ('Epoch [{}/{}], Averge Loss:for epoch {}: {:.4f}]'.format(epoch+1, num_epochs, epoch+1, avg_loss_epoch))\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "neMZnyJ5lp7o"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, test_dataloader, device, input_size):\n",
        "    \"\"\"\n",
        "    Test the trained model performance on test dataset\n",
        "    Args:\n",
        "        model (torchvision.models): model to train\n",
        "        test_dataloader (torch.utils.data.DataLoader): test dataloader\n",
        "    Returns:\n",
        "        model (torchvision.models): trained model\n",
        "    \"\"\"\n",
        "    ## Load the model to GPU if available\n",
        "    model = model.to(device)\n",
        "\n",
        "    ## Set model to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0.\n",
        "    total = 0.\n",
        "\n",
        "    ## Iterate through test dataset\n",
        "    for images, labels in test_dataloader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        ## Reshape images\n",
        "        images = images.reshape(-1, input_size)\n",
        "        \n",
        "        ## Forward pass\n",
        "        outputs = model(images) \n",
        "        \n",
        "        ## Get predictions\n",
        "        predicted = torch.argmax(outputs, axis=1)\n",
        "\n",
        "        ## Calculate accuracy\n",
        "        total += labels.size(0) \n",
        "        correct += (predicted == labels).sum()    \n",
        "\n",
        "    print('Accuracy of the SVM model on the val images: %f %%' % (100 * (correct.float() / total)))"
      ],
      "metadata": {
        "id": "BDNLhm_olvOU"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Torch parameters being used\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "\n",
        "np.random.seed(0)\n",
        "random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "class SVM_Loss(torch.nn.modules.Module):\n",
        "    \"\"\"\n",
        "    SVM Loss function\n",
        "    \"\"\"    \n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the SVM Loss function\n",
        "        \"\"\"\n",
        "        super(SVM_Loss,self).__init__()\n",
        "\n",
        "    def forward(self, outputs, labels, batch_size):\n",
        "        \"\"\"\n",
        "        Forward pass of the SVM Loss function\n",
        "        \"\"\"\n",
        "        return torch.sum(torch.clamp(1 - outputs.t()*labels, min=0))/batch_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwJbiAzIlwNa",
        "outputId": "7a5eac8f-38a6-45bf-b958-8acaf1048d1c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch:  2.0 ; cuda:  cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "svm_model = nn.Linear(input_size,num_classes)\n",
        "\n",
        "  ## Loss and optimizer\n",
        "svm_loss_criteria = SVM_Loss()\n",
        "svm_optimizer = torch.optim.SGD(svm_model.parameters(), lr=learning_rate, momentum=momentum)\n",
        "total_step = len(dataloaders[\"train\"])\n",
        "\n",
        "    ## Train model\n",
        "model = train_model(svm_model, input_size, svm_loss_criteria, svm_optimizer, dataloaders, batch_size, device, num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTFAFoBkmIpH",
        "outputId": "0fdca66e-ba49-448f-b746-9a2acd21cf1d"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Averge Loss:for epoch 1: 1.0993]\n",
            "Epoch [2/50], Averge Loss:for epoch 2: 0.8413]\n",
            "Epoch [3/50], Averge Loss:for epoch 3: 0.8413]\n",
            "Epoch [4/50], Averge Loss:for epoch 4: 0.8413]\n",
            "Epoch [5/50], Averge Loss:for epoch 5: 0.8413]\n",
            "Epoch [6/50], Averge Loss:for epoch 6: 0.8413]\n",
            "Epoch [7/50], Averge Loss:for epoch 7: 0.8413]\n",
            "Epoch [8/50], Averge Loss:for epoch 8: 0.8413]\n",
            "Epoch [9/50], Averge Loss:for epoch 9: 0.8413]\n",
            "Epoch [10/50], Averge Loss:for epoch 10: 0.8413]\n",
            "Epoch [11/50], Averge Loss:for epoch 11: 0.8413]\n",
            "Epoch [12/50], Averge Loss:for epoch 12: 0.8413]\n",
            "Epoch [13/50], Averge Loss:for epoch 13: 0.8413]\n",
            "Epoch [14/50], Averge Loss:for epoch 14: 0.8413]\n",
            "Epoch [15/50], Averge Loss:for epoch 15: 0.8413]\n",
            "Epoch [16/50], Averge Loss:for epoch 16: 0.8413]\n",
            "Epoch [17/50], Averge Loss:for epoch 17: 0.8413]\n",
            "Epoch [18/50], Averge Loss:for epoch 18: 0.8413]\n",
            "Epoch [19/50], Averge Loss:for epoch 19: 0.8413]\n",
            "Epoch [20/50], Averge Loss:for epoch 20: 0.8413]\n",
            "Epoch [21/50], Averge Loss:for epoch 21: 0.8413]\n",
            "Epoch [22/50], Averge Loss:for epoch 22: 0.8413]\n",
            "Epoch [23/50], Averge Loss:for epoch 23: 0.8413]\n",
            "Epoch [24/50], Averge Loss:for epoch 24: 0.8413]\n",
            "Epoch [25/50], Averge Loss:for epoch 25: 0.8413]\n",
            "Epoch [26/50], Averge Loss:for epoch 26: 0.8413]\n",
            "Epoch [27/50], Averge Loss:for epoch 27: 0.8413]\n",
            "Epoch [28/50], Averge Loss:for epoch 28: 0.8413]\n",
            "Epoch [29/50], Averge Loss:for epoch 29: 0.8413]\n",
            "Epoch [30/50], Averge Loss:for epoch 30: 0.8413]\n",
            "Epoch [31/50], Averge Loss:for epoch 31: 0.8413]\n",
            "Epoch [32/50], Averge Loss:for epoch 32: 0.8413]\n",
            "Epoch [33/50], Averge Loss:for epoch 33: 0.8413]\n",
            "Epoch [34/50], Averge Loss:for epoch 34: 0.8413]\n",
            "Epoch [35/50], Averge Loss:for epoch 35: 0.8413]\n",
            "Epoch [36/50], Averge Loss:for epoch 36: 0.8413]\n",
            "Epoch [37/50], Averge Loss:for epoch 37: 0.8413]\n",
            "Epoch [38/50], Averge Loss:for epoch 38: 0.8413]\n",
            "Epoch [39/50], Averge Loss:for epoch 39: 0.8413]\n",
            "Epoch [40/50], Averge Loss:for epoch 40: 0.8413]\n",
            "Epoch [41/50], Averge Loss:for epoch 41: 0.8413]\n",
            "Epoch [42/50], Averge Loss:for epoch 42: 0.8413]\n",
            "Epoch [43/50], Averge Loss:for epoch 43: 0.8413]\n",
            "Epoch [44/50], Averge Loss:for epoch 44: 0.8413]\n",
            "Epoch [45/50], Averge Loss:for epoch 45: 0.8413]\n",
            "Epoch [46/50], Averge Loss:for epoch 46: 0.8413]\n",
            "Epoch [47/50], Averge Loss:for epoch 47: 0.8413]\n",
            "Epoch [48/50], Averge Loss:for epoch 48: 0.8413]\n",
            "Epoch [49/50], Averge Loss:for epoch 49: 0.8413]\n",
            "Epoch [50/50], Averge Loss:for epoch 50: 0.8413]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_dir = '/content/WBC-Differential-Learning-Tool/models'\n",
        "filename = 'SVM_SGD.pt'\n",
        "\n",
        "# Save the entire model\n",
        "torch.save(model, os.path.join(model_dir,filename))"
      ],
      "metadata": {
        "id": "9DFDF_Fh_EcH"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load(\"/content/WBC-Differential-Learning-Tool/models/SVM_SGD.pt\")\n",
        "test_model(model, dataloaders[\"test\"], device, input_size)"
      ],
      "metadata": {
        "id": "FT6daFArlUxX"
      },
      "execution_count": 44,
      "outputs": []
    }
  ]
}